dataset_name: Anthropic/hh-rlhf
model_name_or_path: EleutherAI/pythia-1.4b
per_device_train_batch_size: 8
per_device_eval_batch_size: 16
learning_rate: 5.0e-6
gradient_accumulation_steps: 1
num_train_epochs: 1
logging_steps: 10
eval_strategy: steps
eval_steps: 200
save_strategy: steps
save_steps: 400
run_name: pt-1.4b-hh-dpo-chat
output_dir: outputs/pt-1.4b-hh-dpo-chat
warmup_ratio: 0.1
remove_unused_columns: False
report_to: wandb
bf16: True
dataset_num_proc: 32
use_peft: False